{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. modified all(long description won't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "#import numexpr as ne\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "conn = MySQLdb.connect(\n",
    "                     host='localhost', port=3306, user='***', passwd='***', db = 'eclipse2011',\n",
    "                     charset='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45764, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_info = pd.read_sql(\"SELECT bug_id, short_desc, product_id, component_id, priority FROM bugs WHERE bug_id >= 214049 and bug_id <= 259814;\", conn)\n",
    "bug_info.set_index('bug_id', inplace=True)\n",
    "bug_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>component</td>\n",
       "      <td>Component</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>priority</td>\n",
       "      <td>Priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>product</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>short_desc</td>\n",
       "      <td>Summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        name description\n",
       "0  14   component   Component\n",
       "1  13    priority    Priority\n",
       "2   3     product     Product\n",
       "3   2  short_desc     Summary"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('SELECT id, name, description from fielddefs where name in (\"short_desc\", \"product\", \"component\", \"priority\")', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bug_info.to_pickle(\"original_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('original_info.pkl').reset_index()\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_bigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(1, len(l)):\n",
    "            bigrams.append(l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_trigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(2, len(l)):\n",
    "            bigrams.append(l[i-2] + ' ' + l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "short_desc_bag = df['short_desc'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "short_desc_bag.to_pickle('short_desc_bag.pkl')\n",
    "short_desc_bag = None\n",
    "\n",
    "short_desc_bi_bag = df['short_desc'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "short_desc_bi_bag.to_pickle('short_desc_bi_bag.pkl')\n",
    "short_desc_bi_bag = None\n",
    "\n",
    "short_desc_tri_bag = df['short_desc'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "short_desc_tri_bag.to_pickle('short_desc_tri_bag.pkl')\n",
    "short_desc_tri_bag = None\n",
    "\n",
    "df.to_pickle('bug_info.pkl')\n",
    "df = None\n",
    "\n",
    "description = pd.read_sql('select longdescs.bug_id, longdescs.thetext from longdescs, (select bug_id, MIN(comment_id) as comment_id from longdescs group by bug_id) as t where longdescs.bug_id = t.bug_id and longdescs.comment_id = t.comment_id and longdescs.bug_id >= 214049 and longdescs.bug_id <= 259814', conn)\n",
    "\n",
    "description_bag = description['thetext'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "description_bag.to_pickle('description_bag.pkl')\n",
    "description_bag = None\n",
    "\n",
    "description_bi_bag = description['thetext'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "description_bi_bag.to_pickle('description_bi_bag.pkl')\n",
    "description_bi_bag = None\n",
    "\n",
    "description_tri_bag = description['thetext'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "description_tri_bag.to_pickle('description_tri_bag.pkl')\n",
    "description_tri_bag = None\n",
    "\n",
    "description.to_pickle('description.pkl')\n",
    "description = None\n",
    "\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "all_words = set()\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    for data in series:\n",
    "        for word in data:\n",
    "            all_words.add(word)\n",
    "            \n",
    "dictionary = dict((v,k) for k, v in enumerate(all_words))\n",
    "\n",
    "all_words = None\n",
    "\n",
    "import pickle\n",
    "f = open('dictionary.pkl', 'wb')\n",
    "pickle.dump(dictionary, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle('bug_info.pkl')\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    df[filename[:-4]] = series\n",
    "\n",
    "modified = df['priority'].replace('--', 6).replace('P1', 1).replace('P2', 2).replace('P3', 3).replace('P4', 4).replace('P5', 5)\n",
    "df['priority'] = modified\n",
    "df.to_pickle('all_in_one.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "df = pd.read_pickle('all_in_one.pkl')\n",
    "df.set_index('bug_id', inplace=True)\n",
    "lines = open('large-eclipse.txt').readlines()\n",
    "\n",
    "#ID   0\n",
    "#S-U  1\n",
    "#S-B  2\n",
    "#S-T  3\n",
    "#D-U  4\n",
    "#D-B  5\n",
    "#D-T  6\n",
    "#A-U  7\n",
    "#A-B  8\n",
    "#A-T  9\n",
    "#DID  10\n",
    "#VERSION       11\n",
    "#COMPONENT     12\n",
    "#SUB_COMPONENT 13\n",
    "#TYPE          14\n",
    "#PRIORITY      15\n",
    "with open('eclipse_modified_all.txt','w') as out:\n",
    "    for i in range(0, len(lines), 16):\n",
    "        ID = int(lines[i].strip().split('=')[1])\n",
    "        id_line = lines[i].strip()\n",
    "        did_line = lines[i + 10].strip()\n",
    "        version_line = lines[i + 11].strip()\n",
    "        type_line = lines[i + 14].strip()\n",
    "        if ID in df.index:\n",
    "            out.write(id_line)\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( did_line)\n",
    "            out.write('\\n')\n",
    "            out.write( version_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"COMPONENT=\" + str(df.ix[ID, 'product_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( \"SUB-COMPONENT=\" + str(df.ix[ID, 'component_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( type_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"PRIORITY=\" + str(df.ix[ID, 'priority']))\n",
    "            out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. initial summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "conn = MySQLdb.connect(\n",
    "                     host='localhost', port=3306, user='***', passwd='***', db = 'eclipse2011',\n",
    "                     charset='utf8')\n",
    "\n",
    "bug_info = pd.read_sql(\"SELECT bug_id, short_desc, product_id, component_id, priority FROM bugs WHERE bug_id >= 214049 and bug_id <= 259814;\", conn)\n",
    "bug_info.set_index('bug_id', inplace=True)\n",
    "activities = pd.read_sql('SELECT * FROM bugs_activity where bug_id >= 214049 and bug_id <= 259814 and fieldid = 2 ' , conn)\n",
    "\n",
    "product_df = pd.read_sql('SELECT  name,id FROM products ', conn)\n",
    "product_df.set_index('name', inplace=True)\n",
    "product_mapping = product_df['id'].to_dict()\n",
    "\n",
    "component_df = pd.read_sql('SELECT  name,id FROM components ', conn)\n",
    "component_df.set_index('name', inplace=True)\n",
    "component_mapping = component_df['id'].to_dict()\n",
    "\n",
    "activities.sort('bug_when', ascending=False, inplace=True)\n",
    "\n",
    "after = bug_info.copy()\n",
    "\n",
    "for rownum in range(0, activities.shape[0]):\n",
    "    row = activities.iloc[rownum]\n",
    "    bug_id = row['bug_id']\n",
    "    field = row['fieldid']\n",
    "    removed = row['removed']\n",
    "    try:\n",
    "        if field == 2:\n",
    "            after.ix[bug_id, 'short_desc'] = removed\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "df = after.copy().reset_index()\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_bigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(1, len(l)):\n",
    "            bigrams.append(l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_trigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(2, len(l)):\n",
    "            bigrams.append(l[i-2] + ' ' + l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "short_desc_bag = df['short_desc'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "short_desc_bag.to_pickle('short_desc_bag.pkl')\n",
    "short_desc_bag = None\n",
    "\n",
    "short_desc_bi_bag = df['short_desc'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "short_desc_bi_bag.to_pickle('short_desc_bi_bag.pkl')\n",
    "short_desc_bi_bag = None\n",
    "\n",
    "short_desc_tri_bag = df['short_desc'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "short_desc_tri_bag.to_pickle('short_desc_tri_bag.pkl')\n",
    "short_desc_tri_bag = None\n",
    "\n",
    "df.to_pickle('bug_info.pkl')\n",
    "df = None\n",
    "\n",
    "description = pd.read_sql('select longdescs.bug_id, longdescs.thetext from longdescs, (select bug_id, MIN(comment_id) as comment_id from longdescs group by bug_id) as t where longdescs.bug_id = t.bug_id and longdescs.comment_id = t.comment_id and longdescs.bug_id >= 214049 and longdescs.bug_id <= 259814', conn)\n",
    "\n",
    "description_bag = description['thetext'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "description_bag.to_pickle('description_bag.pkl')\n",
    "description_bag = None\n",
    "\n",
    "description_bi_bag = description['thetext'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "description_bi_bag.to_pickle('description_bi_bag.pkl')\n",
    "description_bi_bag = None\n",
    "\n",
    "description_tri_bag = description['thetext'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "description_tri_bag.to_pickle('description_tri_bag.pkl')\n",
    "description_tri_bag = None\n",
    "\n",
    "description.to_pickle('description.pkl')\n",
    "description = None\n",
    "\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "all_words = set()\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    for data in series:\n",
    "        for word in data:\n",
    "            all_words.add(word)\n",
    "            \n",
    "dictionary = dict((v,k) for k, v in enumerate(all_words))\n",
    "\n",
    "all_words = None\n",
    "\n",
    "import pickle\n",
    "f = open('dictionary.pkl', 'wb')\n",
    "pickle.dump(dictionary, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle('bug_info.pkl')\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    df[filename[:-4]] = series\n",
    "\n",
    "modified = df['priority'].replace('--', 6).replace('P1', 1).replace('P2', 2).replace('P3', 3).replace('P4', 4).replace('P5', 5)\n",
    "df['priority'] = modified\n",
    "\n",
    "df.to_pickle('all_in_one.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "df = pd.read_pickle('all_in_one.pkl')\n",
    "df.set_index('bug_id', inplace=True)\n",
    "lines = open('large-eclipse.txt').readlines()\n",
    "\n",
    "#ID   0\n",
    "#S-U  1\n",
    "#S-B  2\n",
    "#S-T  3\n",
    "#D-U  4\n",
    "#D-B  5\n",
    "#D-T  6\n",
    "#A-U  7\n",
    "#A-B  8\n",
    "#A-T  9\n",
    "#DID  10\n",
    "#VERSION       11\n",
    "#COMPONENT     12\n",
    "#SUB_COMPONENT 13\n",
    "#TYPE          14\n",
    "#PRIORITY      15\n",
    "with open('eclipse_initial_summary.txt','w') as out:\n",
    "    for i in range(0, len(lines), 16):\n",
    "        ID = int(lines[i].strip().split('=')[1])\n",
    "        id_line = lines[i].strip()\n",
    "        did_line = lines[i + 10].strip()\n",
    "        version_line = lines[i + 11].strip()\n",
    "        type_line = lines[i + 14].strip()\n",
    "        if ID in df.index:\n",
    "            out.write(id_line)\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( did_line)\n",
    "            out.write('\\n')\n",
    "            out.write( version_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"COMPONENT=\" + str(df.ix[ID, 'product_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( \"SUB-COMPONENT=\" + str(df.ix[ID, 'component_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( type_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"PRIORITY=\" + str(df.ix[ID, 'priority']))\n",
    "            out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. initial product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "conn = MySQLdb.connect(\n",
    "                     host='localhost', port=3306, user='***', passwd='***', db = 'eclipse2011',\n",
    "                     charset='utf8')\n",
    "\n",
    "bug_info = pd.read_sql(\"SELECT bug_id, short_desc, product_id, component_id, priority FROM bugs WHERE bug_id >= 214049 and bug_id <= 259814;\", conn)\n",
    "bug_info.set_index('bug_id', inplace=True)\n",
    "activities = pd.read_sql('SELECT * FROM bugs_activity where bug_id >= 214049 and bug_id <= 259814 and fieldid = 3 ' , conn)\n",
    "\n",
    "product_df = pd.read_sql('SELECT  name,id FROM products ', conn)\n",
    "product_df.set_index('name', inplace=True)\n",
    "product_mapping = product_df['id'].to_dict()\n",
    "\n",
    "component_df = pd.read_sql('SELECT  name,id FROM components ', conn)\n",
    "component_df.set_index('name', inplace=True)\n",
    "component_mapping = component_df['id'].to_dict()\n",
    "\n",
    "activities.sort('bug_when', ascending=False, inplace=True)\n",
    "\n",
    "after = bug_info.copy()\n",
    "\n",
    "for rownum in range(0, activities.shape[0]):\n",
    "    row = activities.iloc[rownum]\n",
    "    bug_id = row['bug_id']\n",
    "    field = row['fieldid']\n",
    "    removed = row['removed']\n",
    "    try:\n",
    "        if field == 3:\n",
    "            after.ix[bug_id, 'product_id'] = product_mapping[removed]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "df = after.copy().reset_index()\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_bigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(1, len(l)):\n",
    "            bigrams.append(l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_trigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(2, len(l)):\n",
    "            bigrams.append(l[i-2] + ' ' + l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "short_desc_bag = df['short_desc'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "short_desc_bag.to_pickle('short_desc_bag.pkl')\n",
    "short_desc_bag = None\n",
    "\n",
    "short_desc_bi_bag = df['short_desc'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "short_desc_bi_bag.to_pickle('short_desc_bi_bag.pkl')\n",
    "short_desc_bi_bag = None\n",
    "\n",
    "short_desc_tri_bag = df['short_desc'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "short_desc_tri_bag.to_pickle('short_desc_tri_bag.pkl')\n",
    "short_desc_tri_bag = None\n",
    "\n",
    "df.to_pickle('bug_info.pkl')\n",
    "df = None\n",
    "\n",
    "description = pd.read_sql('select longdescs.bug_id, longdescs.thetext from longdescs, (select bug_id, MIN(comment_id) as comment_id from longdescs group by bug_id) as t where longdescs.bug_id = t.bug_id and longdescs.comment_id = t.comment_id and longdescs.bug_id >= 214049 and longdescs.bug_id <= 259814', conn)\n",
    "\n",
    "description_bag = description['thetext'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "description_bag.to_pickle('description_bag.pkl')\n",
    "description_bag = None\n",
    "\n",
    "description_bi_bag = description['thetext'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "description_bi_bag.to_pickle('description_bi_bag.pkl')\n",
    "description_bi_bag = None\n",
    "\n",
    "description_tri_bag = description['thetext'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "description_tri_bag.to_pickle('description_tri_bag.pkl')\n",
    "description_tri_bag = None\n",
    "\n",
    "description.to_pickle('description.pkl')\n",
    "description = None\n",
    "\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "all_words = set()\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    for data in series:\n",
    "        for word in data:\n",
    "            all_words.add(word)\n",
    "            \n",
    "dictionary = dict((v,k) for k, v in enumerate(all_words))\n",
    "\n",
    "all_words = None\n",
    "\n",
    "import pickle\n",
    "f = open('dictionary.pkl', 'wb')\n",
    "pickle.dump(dictionary, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle('bug_info.pkl')\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    df[filename[:-4]] = series\n",
    "\n",
    "modified = df['priority'].replace('--', 6).replace('P1', 1).replace('P2', 2).replace('P3', 3).replace('P4', 4).replace('P5', 5)\n",
    "df['priority'] = modified\n",
    "\n",
    "df.to_pickle('all_in_one.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "df = pd.read_pickle('all_in_one.pkl')\n",
    "df.set_index('bug_id', inplace=True)\n",
    "lines = open('large-eclipse.txt').readlines()\n",
    "\n",
    "#ID   0\n",
    "#S-U  1\n",
    "#S-B  2\n",
    "#S-T  3\n",
    "#D-U  4\n",
    "#D-B  5\n",
    "#D-T  6\n",
    "#A-U  7\n",
    "#A-B  8\n",
    "#A-T  9\n",
    "#DID  10\n",
    "#VERSION       11\n",
    "#COMPONENT     12\n",
    "#SUB_COMPONENT 13\n",
    "#TYPE          14\n",
    "#PRIORITY      15\n",
    "with open('eclipse_initial_product.txt','w') as out:\n",
    "    for i in range(0, len(lines), 16):\n",
    "        ID = int(lines[i].strip().split('=')[1])\n",
    "        id_line = lines[i].strip()\n",
    "        did_line = lines[i + 10].strip()\n",
    "        version_line = lines[i + 11].strip()\n",
    "        type_line = lines[i + 14].strip()\n",
    "        if ID in df.index:\n",
    "            out.write(id_line)\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( did_line)\n",
    "            out.write('\\n')\n",
    "            out.write( version_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"COMPONENT=\" + str(df.ix[ID, 'product_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( \"SUB-COMPONENT=\" + str(df.ix[ID, 'component_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( type_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"PRIORITY=\" + str(df.ix[ID, 'priority']))\n",
    "            out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. initial component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "conn = MySQLdb.connect(\n",
    "                     host='localhost', port=3306, user='***', passwd='***', db = 'eclipse2011',\n",
    "                     charset='utf8')\n",
    "\n",
    "bug_info = pd.read_sql(\"SELECT bug_id, short_desc, product_id, component_id, priority FROM bugs WHERE bug_id >= 214049 and bug_id <= 259814;\", conn)\n",
    "bug_info.set_index('bug_id', inplace=True)\n",
    "activities = pd.read_sql('SELECT * FROM bugs_activity where bug_id >= 214049 and bug_id <= 259814 and fieldid = 14 ' , conn)\n",
    "\n",
    "product_df = pd.read_sql('SELECT  name,id FROM products ', conn)\n",
    "product_df.set_index('name', inplace=True)\n",
    "product_mapping = product_df['id'].to_dict()\n",
    "\n",
    "component_df = pd.read_sql('SELECT  name,id FROM components ', conn)\n",
    "component_df.set_index('name', inplace=True)\n",
    "component_mapping = component_df['id'].to_dict()\n",
    "\n",
    "activities.sort('bug_when', ascending=False, inplace=True)\n",
    "\n",
    "after = bug_info.copy()\n",
    "\n",
    "for rownum in range(0, activities.shape[0]):\n",
    "    row = activities.iloc[rownum]\n",
    "    bug_id = row['bug_id']\n",
    "    field = row['fieldid']\n",
    "    removed = row['removed']\n",
    "    try:\n",
    "        if field == 14:\n",
    "            after.ix[bug_id, 'component_id'] = component_mapping[removed]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "df = after.copy().reset_index()\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_bigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(1, len(l)):\n",
    "            bigrams.append(l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_trigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(2, len(l)):\n",
    "            bigrams.append(l[i-2] + ' ' + l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "short_desc_bag = df['short_desc'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "short_desc_bag.to_pickle('short_desc_bag.pkl')\n",
    "short_desc_bag = None\n",
    "\n",
    "short_desc_bi_bag = df['short_desc'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "short_desc_bi_bag.to_pickle('short_desc_bi_bag.pkl')\n",
    "short_desc_bi_bag = None\n",
    "\n",
    "short_desc_tri_bag = df['short_desc'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "short_desc_tri_bag.to_pickle('short_desc_tri_bag.pkl')\n",
    "short_desc_tri_bag = None\n",
    "\n",
    "df.to_pickle('bug_info.pkl')\n",
    "df = None\n",
    "\n",
    "description = pd.read_sql('select longdescs.bug_id, longdescs.thetext from longdescs, (select bug_id, MIN(comment_id) as comment_id from longdescs group by bug_id) as t where longdescs.bug_id = t.bug_id and longdescs.comment_id = t.comment_id and longdescs.bug_id >= 214049 and longdescs.bug_id <= 259814', conn)\n",
    "\n",
    "description_bag = description['thetext'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "description_bag.to_pickle('description_bag.pkl')\n",
    "description_bag = None\n",
    "\n",
    "description_bi_bag = description['thetext'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "description_bi_bag.to_pickle('description_bi_bag.pkl')\n",
    "description_bi_bag = None\n",
    "\n",
    "description_tri_bag = description['thetext'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "description_tri_bag.to_pickle('description_tri_bag.pkl')\n",
    "description_tri_bag = None\n",
    "\n",
    "description.to_pickle('description.pkl')\n",
    "description = None\n",
    "\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "all_words = set()\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    for data in series:\n",
    "        for word in data:\n",
    "            all_words.add(word)\n",
    "            \n",
    "dictionary = dict((v,k) for k, v in enumerate(all_words))\n",
    "\n",
    "all_words = None\n",
    "\n",
    "import pickle\n",
    "f = open('dictionary.pkl', 'wb')\n",
    "pickle.dump(dictionary, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle('bug_info.pkl')\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    df[filename[:-4]] = series\n",
    "\n",
    "modified = df['priority'].replace('--', 6).replace('P1', 1).replace('P2', 2).replace('P3', 3).replace('P4', 4).replace('P5', 5)\n",
    "df['priority'] = modified\n",
    "\n",
    "df.to_pickle('all_in_one.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "df = pd.read_pickle('all_in_one.pkl')\n",
    "df.set_index('bug_id', inplace=True)\n",
    "lines = open('large-eclipse.txt').readlines()\n",
    "\n",
    "#ID   0\n",
    "#S-U  1\n",
    "#S-B  2\n",
    "#S-T  3\n",
    "#D-U  4\n",
    "#D-B  5\n",
    "#D-T  6\n",
    "#A-U  7\n",
    "#A-B  8\n",
    "#A-T  9\n",
    "#DID  10\n",
    "#VERSION       11\n",
    "#COMPONENT     12\n",
    "#SUB_COMPONENT 13\n",
    "#TYPE          14\n",
    "#PRIORITY      15\n",
    "with open('eclipse_initial_component.txt','w') as out:\n",
    "    for i in range(0, len(lines), 16):\n",
    "        ID = int(lines[i].strip().split('=')[1])\n",
    "        id_line = lines[i].strip()\n",
    "        did_line = lines[i + 10].strip()\n",
    "        version_line = lines[i + 11].strip()\n",
    "        type_line = lines[i + 14].strip()\n",
    "        if ID in df.index:\n",
    "            out.write(id_line)\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( did_line)\n",
    "            out.write('\\n')\n",
    "            out.write( version_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"COMPONENT=\" + str(df.ix[ID, 'product_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( \"SUB-COMPONENT=\" + str(df.ix[ID, 'component_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( type_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"PRIORITY=\" + str(df.ix[ID, 'priority']))\n",
    "            out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5. initial priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "conn = MySQLdb.connect(\n",
    "                     host='localhost', port=3306, user='***', passwd='***', db = 'eclipse2011',\n",
    "                     charset='utf8')\n",
    "\n",
    "bug_info = pd.read_sql(\"SELECT bug_id, short_desc, product_id, component_id, priority FROM bugs WHERE bug_id >= 214049 and bug_id <= 259814;\", conn)\n",
    "bug_info.set_index('bug_id', inplace=True)\n",
    "activities = pd.read_sql('SELECT * FROM bugs_activity where bug_id >= 214049 and bug_id <= 259814 and fieldid = 13 ' , conn)\n",
    "\n",
    "product_df = pd.read_sql('SELECT  name,id FROM products ', conn)\n",
    "product_df.set_index('name', inplace=True)\n",
    "product_mapping = product_df['id'].to_dict()\n",
    "\n",
    "component_df = pd.read_sql('SELECT  name,id FROM components ', conn)\n",
    "component_df.set_index('name', inplace=True)\n",
    "component_mapping = component_df['id'].to_dict()\n",
    "\n",
    "activities.sort('bug_when', ascending=False, inplace=True)\n",
    "\n",
    "after = bug_info.copy()\n",
    "\n",
    "for rownum in range(0, activities.shape[0]):\n",
    "    row = activities.iloc[rownum]\n",
    "    bug_id = row['bug_id']\n",
    "    field = row['fieldid']\n",
    "    removed = row['removed']\n",
    "    try:\n",
    "        if field == 13:\n",
    "            after.ix[bug_id, 'priority'] = removed\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "df = after.copy().reset_index()\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_bigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(1, len(l)):\n",
    "            bigrams.append(l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_trigrams(string):\n",
    "    try:\n",
    "        l = filter(lambda x: x.isalpha(), tokenizer.tokenize(string.lower()))\n",
    "        bigrams = []\n",
    "        for i in range(2, len(l)):\n",
    "            bigrams.append(l[i-2] + ' ' + l[i-1] + ' ' + l[i])\n",
    "        return bigrams\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "short_desc_bag = df['short_desc'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "short_desc_bag.to_pickle('short_desc_bag.pkl')\n",
    "short_desc_bag = None\n",
    "\n",
    "short_desc_bi_bag = df['short_desc'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "short_desc_bi_bag.to_pickle('short_desc_bi_bag.pkl')\n",
    "short_desc_bi_bag = None\n",
    "\n",
    "short_desc_tri_bag = df['short_desc'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "short_desc_tri_bag.to_pickle('short_desc_tri_bag.pkl')\n",
    "short_desc_tri_bag = None\n",
    "\n",
    "df.to_pickle('bug_info.pkl')\n",
    "df = None\n",
    "\n",
    "description = pd.read_sql('select longdescs.bug_id, longdescs.thetext from longdescs, (select bug_id, MIN(comment_id) as comment_id from longdescs group by bug_id) as t where longdescs.bug_id = t.bug_id and longdescs.comment_id = t.comment_id and longdescs.bug_id >= 214049 and longdescs.bug_id <= 259814', conn)\n",
    "\n",
    "description_bag = description['thetext'].apply(lambda string: Counter(filter(lambda x: x not in stopwords and x.isalpha(), map(lambda x: stemmer.stem(x), tokenizer.tokenize(string.lower())))))\n",
    "description_bag.to_pickle('description_bag.pkl')\n",
    "description_bag = None\n",
    "\n",
    "description_bi_bag = description['thetext'].apply(lambda string: Counter(get_bigrams(string)))\n",
    "description_bi_bag.to_pickle('description_bi_bag.pkl')\n",
    "description_bi_bag = None\n",
    "\n",
    "description_tri_bag = description['thetext'].apply(lambda string: Counter(get_trigrams(string)))\n",
    "description_tri_bag.to_pickle('description_tri_bag.pkl')\n",
    "description_tri_bag = None\n",
    "\n",
    "description.to_pickle('description.pkl')\n",
    "description = None\n",
    "\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "all_words = set()\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    for data in series:\n",
    "        for word in data:\n",
    "            all_words.add(word)\n",
    "            \n",
    "dictionary = dict((v,k) for k, v in enumerate(all_words))\n",
    "\n",
    "all_words = None\n",
    "\n",
    "import pickle\n",
    "f = open('dictionary.pkl', 'wb')\n",
    "pickle.dump(dictionary, f)\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle('bug_info.pkl')\n",
    "bags = [None] * 6\n",
    "bags[0] = 'short_desc_bag.pkl'\n",
    "bags[1] = 'short_desc_bi_bag.pkl'\n",
    "bags[2] = 'short_desc_tri_bag.pkl'\n",
    "bags[3] = 'description_bag.pkl'\n",
    "bags[4] = 'description_bi_bag.pkl'\n",
    "bags[5] = 'description_tri_bag.pkl'\n",
    "\n",
    "for filename in bags:\n",
    "    series = pd.read_pickle(filename)\n",
    "    df[filename[:-4]] = series\n",
    "\n",
    "modified = df['priority'].replace('--', 6).replace('P1', 1).replace('P2', 2).replace('P3', 3).replace('P4', 4).replace('P5', 5)\n",
    "df['priority'] = modified\n",
    "\n",
    "df.to_pickle('all_in_one.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('dictionary.pkl', 'rb')\n",
    "dictionary = pickle.load(f)\n",
    "df = pd.read_pickle('all_in_one.pkl')\n",
    "df.set_index('bug_id', inplace=True)\n",
    "lines = open('large-eclipse.txt').readlines()\n",
    "\n",
    "#ID   0\n",
    "#S-U  1\n",
    "#S-B  2\n",
    "#S-T  3\n",
    "#D-U  4\n",
    "#D-B  5\n",
    "#D-T  6\n",
    "#A-U  7\n",
    "#A-B  8\n",
    "#A-T  9\n",
    "#DID  10\n",
    "#VERSION       11\n",
    "#COMPONENT     12\n",
    "#SUB_COMPONENT 13\n",
    "#TYPE          14\n",
    "#PRIORITY      15\n",
    "with open('eclipse_initial_priority.txt','w') as out:\n",
    "    for i in range(0, len(lines), 16):\n",
    "        ID = int(lines[i].strip().split('=')[1])\n",
    "        id_line = lines[i].strip()\n",
    "        did_line = lines[i + 10].strip()\n",
    "        version_line = lines[i + 11].strip()\n",
    "        type_line = lines[i + 14].strip()\n",
    "        if ID in df.index:\n",
    "            out.write(id_line)\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'S-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'D-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-U=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-B=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_bi_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( 'A-T=' + ','.join(map(lambda pair: str(dictionary[pair[0]]) + ':' + str(pair[1]), df.ix[ID, 'short_desc_bag'].items() + df.ix[ID, 'description_tri_bag'].items())))\n",
    "            out.write('\\n')\n",
    "            out.write( did_line)\n",
    "            out.write('\\n')\n",
    "            out.write( version_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"COMPONENT=\" + str(df.ix[ID, 'product_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( \"SUB-COMPONENT=\" + str(df.ix[ID, 'component_id']))\n",
    "            out.write('\\n')\n",
    "            out.write( type_line)\n",
    "            out.write('\\n')\n",
    "            out.write( \"PRIORITY=\" + str(df.ix[ID, 'priority']))\n",
    "            out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
